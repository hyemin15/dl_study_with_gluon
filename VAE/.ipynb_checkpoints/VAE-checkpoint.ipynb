{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference\n",
    "\n",
    "## Variationa Inference 요약\n",
    "\n",
    "  - VAE의 핵심은 Variational Inference(VI)이다. DNN은 Variational Inference를 하기 위한 기존 통계적인 방법들에 비해 아주 큰 해상도를 지니는 하나의 방법일 뿐이다.\n",
    "    * 자주 GAN(Generative Adversarial Network)와 비교되는데 이는 둘 다 비지도 학습(unsupervised learning)이면서, 주어진 정보를 이용해서 무언가 새롭게 만들어낼 수 있는 능력 때문이다.\n",
    "    * GAN이 two-player 개념에 근거하여 새로운 Loss function을 제시하였다면, VAE는 새로운 Loss Function을 제공했다기 보다는 VAE의 목적함수를 Autoencoder 개념 하에서 재해석하였다고 간주할 수 있다.\n",
    "    \n",
    "\n",
    "  - VI를 위해 DNN의 큰 capacity를 이용, hidden variable을 정의하였다는 것이 특징이다.\n",
    "    * Variational Inference의 전신(?)이라고 생각할 수 있는 EM algorithm에 비해 variational inference도 확률모형에 기반하지 않고 분포를 근사하는 용도로만 사용하고 있지만, VAE는 이런 사상을 극대화한 방법이다.\n",
    "    * VAE의 instrumental distribution는 전혀 확률모형에 기반하지 않고 sample이 편한 확률분포이기만 하면 된다.\n",
    "\n",
    "  - DNN을 이용함으로써, SGD 최적화 algorithm 사용하여 쉽게 학습 가능하다.\n",
    "    * VI로 분포를 추정하는 경우, 기존에는 MCMC 등 computing이 많이 필요로 하는 기법을 사용하였지만, VAE의 경우 backpropagation에 근거한 방법들을 쉽게 적용할 수 있다는 장점이 있다.\n",
    "    \n",
    "  - 이렇게 해서 얻은 latent mapping은 Latent variable의 공간에서의 거리가 의미를 지닌다는 장점이 있다.\n",
    "    * 이는 GAN과 마찬가지 성격으로, embedding과도 그 개념이 연결되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률 모형\n",
    "\n",
    "- 결국 통계 분석의 궁극적인 목적은 다음의 우도함수에 대해 속속들이 알아내는 것이다.\n",
    "$$ P(X;\\theta) = L(\\theta; X)$$\n",
    "  * 이것만 알고 있으면, 주어진 data로 알아낼 수 있는 모든 현상은 계산해 낼 수 있다.\n",
    "  * $\\theta$를 알고 있기만 하다면, 확률변수 $X$가 어떤 확률로 어떤 범위에 있을지를 계산할 수도 있다. 이것만 계산할 수 있으면 확률의 세계에서는 모든 것을 알고 있다라고 말한다.\n",
    "  * Frequentist, Fisherian들이 많이 연구하는 모수통계(Parametric statisics)에서는 그러한 확률분포가 몇몇 개의 모수로 표현되는 경우를 상정하고 그 가정 하에서 분포를 찾아내는 작업을, \n",
    "  * 비모수통계(Nonparametric statics)에서는 최소한의 가정만으로 이를 찾아내고자 한다.\n",
    "  * 또 하나의 극단은, 이런 확률 분포를 sample로 가지고 있는 경우로, Bayesian들이 주로 택하는 방법이다.\n",
    "  * 모수적인 방법은 이론적으로 아름답지만, 다룰 수 있는 문제의 범위가 작고, 그다음으로 비모수 통계는 문제의 범위가 작으며, Sampling 방법에 의존한 방법들은 computing power만 허락한다면, 거의 모든 문제를 다룰 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률 모형의 근사\n",
    "\n",
    "- 하지만, 많은 경우, 우도함수가 아주 복잡한 함수일 경우가 많아, 이를 바로 추정하는 것은 계산상 거의 불가능한 일인 경우가 많이 있다.\n",
    "  * Sampling 방법을 사용할 수 있기는 하지만, sampling 방법은 시간이 많이 든다는 커다란 단점이 있다.\n",
    "  * 이를 해결하기 위해서 Bayesian에서도 근사법(approximation method)를 사용한다.\n",
    "  \n",
    "- 근사하는 방법에는 Laplace approximation 등의 방법이 존재하기는 하지만, 나중에 Bayesian에서 사용하는 VI의 모태가 되는 idea로 EM algorithm(Dempster et al. 1977)이 있다. \n",
    "  * 만약에 실제 관측되는 변수의 생성원리(probabilitistic mechanism)를 잘 설명할 수 있는 잠재변수(Latent variable)을 알 수 있다면, 위의 Likelihood는 다음과 같이 쓸 수 있다.\n",
    "$$L(\\theta; x)= p(x|\\theta)=\\sum_z p(x,z|\\theta)=\\int_\\Omega p(x,z|\\theta)dz$$\n",
    "  * 이런 모형이 상정된 이상, 1.2에 정의한 $L(\\theta; X)$를 marginal likelihood라고 부른다.\n",
    "  * $p(x, z)$를 안다는 것은 $p(x|z)$를 알고, $p(z)$를 안다는 것과 동치이다.\n",
    "  * $P(x|z)$는 확률 모형으로 설정하여 추정을 하고, $p(z)$는 보통 가정을 하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM algorithm\n",
    "\n",
    "- EM algorithm을 적용할 수 있는 문제가 그렇게 많지는 않다.\n",
    "  * EM algorithm을 만들 수 있으려면, 다음의 두가지가 만족이 되어야 한다.\n",
    "    * 관측되는 현상을 만들어내는 원리를 묘사할 수 있는 잠재변수(Latent variable)과 이를 연결시켜줄 수 있는 확률모형이 존재해야 한다.\n",
    "    * 그러한 상황에서 Latent variable을 추정할 수 있어야 하고, 무엇보다도 문제를 간소화 할 수 있어야 한다.\n",
    "  * 하지만, 위의 조건을 만족하는 모형은 그렇게 많지 않다.\n",
    "    * 가장 많이 알려진 모형은 mixture model이다.\n",
    "    * mixture model은 k-means clustering의 일반화된 형태라고 생각할 수 있다. 결국 mixture 모형을 극도로 단순하게 만들어 놓은 것이 k-means이고, k-means algorithm은 EM algorithm의 가장 간단한 형태라고 생각할 수 있다. 뒷부분에서 이야기 될 것이다.\n",
    "    * 사례는 적극적으로 찾아보지는 못했지만, 그 이외에 적용하는 사례는 EM algorithm가 일반화된 형태인 MM algorithm을 연구하는 school 밖에서는 크게 찾아보기 어렵다. 앞으로 찾게 되면 소개하도록 하겠다.\n",
    "    * Mixture model은 관측되는 확률 변수가 간단한 여러 확률 변수가 섞여 있는 형태로 가정한다. 이 예를 통해 EM algorithm이 어떻게 구현되는지 살펴 본다.\n",
    "    \n",
    "- Finite Gaussian Mixture Model\n",
    "\n",
    "$$p(x_i|\\theta)=\\sum_{k=1}^K \\pi_k N(x_i|\\mu_k,\\Sigma_k), \\quad \\sum_{k=1}^K \\pi_k=1$$\n",
    "  * 가장 쉬운 형태의 mixture model이다.\n",
    "  * 위와 같이 새로운 확률변수 $p(x_i|\\theta)$를 만들어도 이는 확률변수의 공리(axiom)을 만족한다.\n",
    "    * 당연히 모든 event에 대한 확률값은 0보다 크다.\n",
    "    * 그리고 모든 event에 대해 확률값을 더했을 때 1이 된다.\n",
    "$$\\int f(x|\\theta) dx =\\int \\sum_{k=1}^K \\pi_k f_k(x|\\mu_k,\\Sigma_k) dx = \\sum_{k=1}^K\\int f_k(x|\\mu_k,\\Sigma_k) dx=1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:kion_venv_mxnet]",
   "language": "python",
   "name": "conda-env-kion_venv_mxnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
